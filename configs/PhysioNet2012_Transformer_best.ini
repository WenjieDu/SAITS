[file_path]
; prefix of saving dir
prefix = .
; base dir, in which the dataset is saved
dataset_base_dir = generated_datasets
result_saving_base_dir = NIPS_results

; Below items are for testing
; dir to save models
model_saving_dir = ${prefix}/${result_saving_base_dir}/${model:model_name}/models/2021-04-22_T02:45:33
; dir to save graphs, which will be plotted in model testing
test_results_saving_base_dir = ${prefix}/NIPS_results

[dataset]
; dir name of your dataset, dataset_base_dir/dataset_name should be the absolute path of dataset
dataset_name = physio2012_37feats_01masked
; sequence length, namely the num of past days
seq_len = 48
; num of input features
feature_num = 37
; batch size
batch_size = 128
; num of workers in dataloader
num_workers = 4
; evaluate every n steps
eval_every_n_steps = 60

[model]
; name of your model, will be the name of dir to save your models and logs
model_name = PhysioNet2012_Transformer_best
; whether concat input with missing mask
input_with_mask = True
; model type, Transformer/SAITS
model_type = Transformer
; num of layer groups
n_groups = 3
; num of group-inner layers
n_group_inner_layers = 1
; how to share parameters, inner_group/between_group
param_sharing_strategy = inner_group
; model hidden dim
d_model = 256
; hidden size of feed forward layer
d_inner = 2048
; head num of self-attention
n_head = 8
; key dim
d_k = 32
; value dim
d_v = 64
; drop out rate
dropout = 0
; whether to apply diagonal attention mask
diagonal_attention_mask = False

[training]
; whether to have Masked Imputation Task (MIT) in training
MIT = True
; whether to have Observed Reconstruction Task (ORT) in training
ORT = True
; max num of training epochs
epochs = 10000
; which device for training, cpu/cuda
device = cuda
; learning rate
lr = 0.000669204734411692
; weight for reconstruction loss
reconstruction_loss_weight = 1
; weight for imputation loss
imputation_loss_weight = 1
; patience of early stopping, -1 means not applied (current early stopping is based on total loss)
early_stop_patience = 30
; what type of optimizer to use, adam/adamw
optimizer_type = adam
; weight decay used in optimizer
weight_decay = 0
; max_norm for gradient clipping, set 0 to disable
max_norm = 0
; strategy on model saving, all/best/none. If set as none, then do not save models (mode for hyper-parameter searching)
model_saving_strategy = best

[test]
; whether to save imputed data
save_imputations = True
; name of model your select for testing
step_184 = model_trainStep_11040_valStep_184_imputationMAE_0.1893
; absolute path to locate model you select
model_path = ${file_path:model_saving_dir}/${step_184}
; path of dir to save generated figs (PR-curve etc.)
result_saving_path = ${file_path:test_results_saving_base_dir}/${model:model_name}/step_184